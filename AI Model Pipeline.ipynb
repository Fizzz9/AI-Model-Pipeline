{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca2d044-7a41-4941-8b20-f7d2e1db0c14",
   "metadata": {},
   "source": [
    "# Step 1: Environment setup\n",
    "\n",
    "Install required libraries and set up the PyTorch device (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcab8f41-87eb-4635-949e-69960aedee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\n",
      "Torch version: 2.3.1+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required libraries (run once)\n",
    "!pip install -q transformers datasets scikit-learn accelerate\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1129fe1-b658-4e47-94f3-56f5af0af211",
   "metadata": {},
   "source": [
    "# Step 2: Load AG News dataset and create splits\n",
    "\n",
    "Load the AG News dataset from HuggingFace Datasets and split it into training, validation, and test sets for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1a21be-ab0a-4a94-83a6-77f452d01f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load AG News dataset from HuggingFace\n",
    "raw_datasets = load_dataset(\"ag_news\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab16a3b-7f00-4736-9246-26040bd24789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108000, 12000, 7600)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train / validation / test splits\n",
    "full_train = raw_datasets[\"train\"]\n",
    "full_test = raw_datasets[\"test\"]\n",
    "\n",
    "# Take 10% of train as validation\n",
    "splits = full_train.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "valid_dataset = splits[\"test\"]\n",
    "test_dataset = full_test  \n",
    "\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b936d-7bf6-4ede-877e-484ded0cbcca",
   "metadata": {},
   "source": [
    "# Step 3: Naïve keyword-based baseline\n",
    "\n",
    "We implement a simple rule-based classifier that assigns each headline\n",
    "to a class based on hand-crafted keyword lists, and evaluate it on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b125497-1733-4df1-a1ad-3e3cd73e2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Define label mappings and keyword lists for the baseline\n",
    "import re\n",
    "\n",
    "# AG News label mapping: 0=World, 1=Sports, 2=Business, 3=Sci/Tech\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "id2label = {i: name for i, name in enumerate(label_names)}\n",
    "label2id = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "# Hand-crafted keyword lists for each class\n",
    "keyword_dict = {\n",
    "    0: [\"war\", \"peace\", \"election\", \"government\", \"president\", \"minister\",\n",
    "        \"attack\", \"conflict\", \"talks\", \"summit\", \"union\", \"party\"],\n",
    "    1: [\"game\", \"match\", \"season\", \"team\", \"coach\", \"league\", \"score\",\n",
    "        \"win\", \"loss\", \"cup\", \"tournament\", \"player\"],\n",
    "    2: [\"stock\", \"market\", \"share\", \"profit\", \"losses\", \"bank\", \"company\",\n",
    "        \"deal\", \"trade\", \"merger\", \"economy\", \"business\"],\n",
    "    3: [\"software\", \"internet\", \"computer\", \"technology\", \"phone\", \"chip\",\n",
    "        \"research\", \"scientist\", \"space\", \"nasa\", \"online\", \"web\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebcbcfe-6409-475e-b5af-53cf4fd0fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Text cleaning and keyword-based prediction function\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase and remove punctuation for simple keyword matching.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def baseline_predict(text: str) -> int:\n",
    "    \"\"\"Return predicted label id using simple keyword counts.\"\"\"\n",
    "    text_clean = clean_text(text)\n",
    "    tokens = text_clean.split()\n",
    "\n",
    "    scores = {label: 0 for label in keyword_dict}\n",
    "    for label, keywords in keyword_dict.items():\n",
    "        for kw in keywords:\n",
    "            if kw in tokens:\n",
    "                scores[label] += 1\n",
    "\n",
    "    # If all scores are zero, fall back to most frequent class in training (World=0)\n",
    "    if all(score == 0 for score in scores.values()):\n",
    "        return 0\n",
    "\n",
    "    # Otherwise choose the class with the highest score\n",
    "    best_label = max(scores, key=scores.get)\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164bb68a-d462-4629-b2d3-d6d0c3715065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.5332\n",
      "Baseline macro-F1: 0.5263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.39      0.92      0.55      1900\n",
      "      Sports       0.82      0.53      0.64      1900\n",
      "    Business       0.60      0.36      0.45      1900\n",
      "    Sci/Tech       0.80      0.33      0.46      1900\n",
      "\n",
      "    accuracy                           0.53      7600\n",
      "   macro avg       0.65      0.53      0.53      7600\n",
      "weighted avg       0.65      0.53      0.53      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Evaluate the keyword baseline on the test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_baseline(dataset, max_samples=None):\n",
    "    texts = dataset[\"text\"]\n",
    "    labels = dataset[\"label\"]\n",
    "\n",
    "    if max_samples is not None:\n",
    "        texts = texts[:max_samples]\n",
    "        labels = labels[:max_samples]\n",
    "\n",
    "    preds = [baseline_predict(t) for t in texts]\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro = f1_score(labels, preds, average=\"macro\")\n",
    "    print(f\"Baseline accuracy: {acc:.4f}\")\n",
    "    print(f\"Baseline macro-F1: {macro:.4f}\")\n",
    "    print()\n",
    "    print(classification_report(labels, preds, target_names=label_names))\n",
    "    return preds\n",
    "\n",
    "baseline_test_preds = evaluate_baseline(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c5c95-b523-40e3-8714-a4b615f6c760",
   "metadata": {},
   "source": [
    "# Step 4: Tokenize the dataset for DistilBERT\n",
    "\n",
    "Prepare a DistilBERT tokenizer and convert the raw text into input IDs, attention masks, and labels so that the model can consume the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01190530-bd1a-444b-aeb1-6fbf587941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Prepare DistilBERT tokenizer and preprocessing function\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = 64  # headlines are short; 64 tokens are enough\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the 'text' field and attach labels for Trainer.\"\"\"\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3322a8db-65bc-47f6-9ac5-c6bf9da039e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "decoded text: despair and anger in small russian town after siege beslan, russia ( reuters ) - the killing of more than 320 children, parents and teachers during the bloody end to a 53 - hour school siege left barely a family untouched in the small russian town of beslan.\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 4.2: Apply tokenization to train/validation/test splits\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_valid = valid_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=valid_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "example = tokenized_train[0]\n",
    "print(example.keys())  \n",
    "print(\"decoded text:\", tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"label:\", example[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930078e-b98d-4af7-aec3-b6c4ccd1ca89",
   "metadata": {},
   "source": [
    "# Step 5: Define DistilBERT model and training setup\n",
    "\n",
    "Instantiate a DistilBERT-based sequence classification model and specify the evaluation metrics (accuracy and macro-F1) together with training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac70ecf3-6e43-4c76-ad38-ec8de5bdffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: distilbert-base-uncased\n",
      "Number of labels: 4\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Step 5.1: Create DistilBERT classification model\n",
    "num_labels = 4\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded model:\", model_name)\n",
    "print(\"Number of labels:\", num_labels)\n",
    "print(\"Model device:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299b171d-3c3f-4880-a998-9e6807123a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Define metric function for Trainer (accuracy + macro-F1)\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy and macro-F1 for evaluation.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef8421-46ab-497a-8b57-529525c7d4df",
   "metadata": {},
   "source": [
    "# Step 6: Fine-tune DistilBERT on AG News\n",
    "\n",
    "Fine-tune the DistilBERT classifier on the tokenized training set, using the validation set to monitor performance and select the best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8953ce5-9a15-4ef7-bbf9-e0cb9fd2b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Step 6.1: Set training hyperparameters (compatible version)\n",
    "batch_size = 32\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    num_train_epochs=5,                     \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,  \n",
    "    warmup_ratio=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e39861d-0e09-41fb-b4a4-498769a89eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Step 6.2: Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a204e0-d856-4f24-ab80-0f891766b740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16875' max='16875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16875/16875 13:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16875, training_loss=0.11609061434710467, metrics={'train_runtime': 816.0729, 'train_samples_per_second': 661.706, 'train_steps_per_second': 20.678, 'total_flos': 8941868328960000.0, 'train_loss': 0.11609061434710467, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6.3: Train the model\n",
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb603b29-26d9-4d81-b074-aec32c546f3e",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate DistilBERT and compare with the keyword baseline\n",
    "\n",
    "We evaluate the fine-tuned DistilBERT model on the validation and test sets,\n",
    "then compare its performance with the naïve keyword-based baseline on the same\n",
    "AG News test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5003c979-579c-4f7f-b68c-f0e1a4b9a355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.3276974856853485, 'eval_accuracy': 0.942, 'eval_macro_f1': 0.9417004095170786, 'eval_runtime': 5.6497, 'eval_samples_per_second': 2123.991, 'eval_steps_per_second': 66.375, 'epoch': 5.0}\n",
      "Test metrics: {'eval_loss': 0.32963278889656067, 'eval_accuracy': 0.9413157894736842, 'eval_macro_f1': 0.9413301112983662, 'eval_runtime': 3.8136, 'eval_samples_per_second': 1992.884, 'eval_steps_per_second': 62.409, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 7.1: Evaluate on validation and test sets\n",
    "val_metrics = trainer.evaluate(tokenized_valid)\n",
    "print(\"Validation metrics:\", val_metrics)\n",
    "\n",
    "test_metrics = trainer.evaluate(tokenized_test)\n",
    "print(\"Test metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037a0cc2-9767-4437-92ed-80d7691940b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.96      0.95      0.95      1900\n",
      "      Sports       0.98      0.99      0.99      1900\n",
      "    Business       0.91      0.92      0.91      1900\n",
      "    Sci/Tech       0.92      0.91      0.91      1900\n",
      "\n",
      "    accuracy                           0.94      7600\n",
      "   macro avg       0.94      0.94      0.94      7600\n",
      "weighted avg       0.94      0.94      0.94      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7.2: Detailed classification report on the test set\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "pred_output = trainer.predict(tokenized_test)\n",
    "test_preds = np.argmax(pred_output.predictions, axis=-1)\n",
    "test_labels = pred_output.label_ids\n",
    "\n",
    "print(classification_report(test_labels, test_preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58978f3-68f2-4406-8cad-6966c78200aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.5332\n",
      "Baseline macro-F1: 0.5263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.39      0.92      0.55      1900\n",
      "      Sports       0.82      0.53      0.64      1900\n",
      "    Business       0.60      0.36      0.45      1900\n",
      "    Sci/Tech       0.80      0.33      0.46      1900\n",
      "\n",
      "    accuracy                           0.53      7600\n",
      "   macro avg       0.65      0.53      0.53      7600\n",
      "weighted avg       0.65      0.53      0.53      7600\n",
      "\n",
      "Baseline accuracy: 0.5332\n",
      "Baseline macro-F1: 0.5263\n"
     ]
    }
   ],
   "source": [
    "# Step 7.3: Recompute baseline metrics for comparison\n",
    "baseline_test_preds = evaluate_baseline(test_dataset)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "baseline_acc = accuracy_score(test_dataset[\"label\"], baseline_test_preds)\n",
    "baseline_macro = f1_score(test_dataset[\"label\"], baseline_test_preds, average=\"macro\")\n",
    "\n",
    "print(f\"Baseline accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"Baseline macro-F1: {baseline_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7682f529-aeba-4f1d-b911-91b1475f15f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test set comparison ===\n",
      "Keyword baseline  - acc: 0.5332, macro-F1: 0.5263\n",
      "DistilBERT model  - acc: 0.9413, macro-F1: 0.9413\n",
      "Accuracy gain     : 0.4082\n",
      "Macro-F1 gain     : 0.4150\n"
     ]
    }
   ],
   "source": [
    "# Step 7.4: Print a small comparison summary\n",
    "bert_acc = test_metrics[\"eval_accuracy\"]\n",
    "bert_macro = test_metrics[\"eval_macro_f1\"]\n",
    "\n",
    "print(\"=== Test set comparison ===\")\n",
    "print(f\"Keyword baseline  - acc: {baseline_acc:.4f}, macro-F1: {baseline_macro:.4f}\")\n",
    "print(f\"DistilBERT model  - acc: {bert_acc:.4f}, macro-F1: {bert_macro:.4f}\")\n",
    "print(f\"Accuracy gain     : {bert_acc - baseline_acc:.4f}\")\n",
    "print(f\"Macro-F1 gain     : {bert_macro - baseline_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acbb83b-ecdd-4eaf-8ec5-13cb2ea930f0",
   "metadata": {},
   "source": [
    "# Step 8: Qualitative examples where baseline and DistilBERT disagree\n",
    "\n",
    "We inspect a few test examples where the keyword-based baseline and the\n",
    "DistilBERT model make different predictions. For each example, we show the\n",
    "headline, the gold label, the baseline prediction, and the DistilBERT\n",
    "prediction, and briefly discuss why the AI model works better (or worse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab1508c-9beb-475c-9c5d-b7c66b414ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8.1: Find indices where baseline and DistilBERT predictions differ\n",
    "import random\n",
    "\n",
    "diff_indices = [\n",
    "    i for i, (b, m) in enumerate(zip(baseline_test_preds, test_preds))\n",
    "    if b != m\n",
    "]\n",
    "\n",
    "len(diff_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12dafa73-ff51-404b-8b85-f18c006cfdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5666, 928, 222, 6596, 2373]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8.2: Sample 3 examples where they disagree\n",
    "sample_indices = random.sample(diff_indices, 5)\n",
    "sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0c9a7b6-e1cf-440f-80ee-2be5f2a2e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Index        : 5666\n",
      "Headline     : Indian PM pledges to protect poor from oil-driven inflation NEW DELHI : Indian Prime Minister Manmohan Singh pledged to try to shield the poor by keeping down prices of essential goods amid rising inflation.\n",
      "Gold label   : Business\n",
      "Baseline pred: World\n",
      "DistilBERT   : Business\n",
      "================================================================================\n",
      "Index        : 928\n",
      "Headline     : Coming to a TV near you: Ads for desktop Linux Linspire CEO points out that recent TV ads serve as indication of acceptance in mainstream populace.\n",
      "Gold label   : Sci/Tech\n",
      "Baseline pred: World\n",
      "DistilBERT   : Sci/Tech\n",
      "================================================================================\n",
      "Index        : 222\n",
      "Headline     : Selling Houston Warts and All, Especially Warts Descriptions of urban afflictions and images of giant mosquitoes and cockroaches to convey a sense of how Houston is nevertheless beloved by many residents.\n",
      "Gold label   : Business\n",
      "Baseline pred: World\n",
      "DistilBERT   : Business\n",
      "================================================================================\n",
      "Index        : 6596\n",
      "Headline     : South Korea, Singapore seal free-trade pact Korea and Singapore sealed a free-trade agreement yesterday that covers nine broad areas, including electronics, finance and intellectual property rights.\n",
      "Gold label   : Business\n",
      "Baseline pred: Business\n",
      "DistilBERT   : World\n",
      "================================================================================\n",
      "Index        : 2373\n",
      "Headline     : US may draw on oil in reserve WASHINGTON Oil prices climbed toward \\$49 per barrel Thursday even as the Bush administration considered drawing crude from the US emergency stockpile and lending it to refiners whose supplies were disrupted by Hurricane Ivan.\n",
      "Gold label   : Business\n",
      "Baseline pred: World\n",
      "DistilBERT   : Business\n"
     ]
    }
   ],
   "source": [
    "# Step 8.3: Print the selected examples\n",
    "for idx in sample_indices:\n",
    "    text = test_dataset[\"text\"][idx]\n",
    "    true_label = label_names[test_dataset[\"label\"][idx]]\n",
    "    base_label = label_names[baseline_test_preds[idx]]\n",
    "    bert_label = label_names[test_preds[idx]]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Index        : {idx}\")\n",
    "    print(f\"Headline     : {text}\")\n",
    "    print(f\"Gold label   : {true_label}\")\n",
    "    print(f\"Baseline pred: {base_label}\")\n",
    "    print(f\"DistilBERT   : {bert_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
